"Accountable"	"getChildResources"	"Returns nested resources of this class. The result should be a point-in-time snapshot (to avoid race conditions)."
"Accountable"	"ramBytesUsed"	"Return the memory usage of this object in bytes. Negative values are illegal."
"Accountables"	"namedAccountables"	"Converts a map of resources to a collection. The resource descriptions are constructed in this format: prefix 'key' [toString()] This is a point-in-time type safe view: consumers will not be able to cast or manipulate the resources in any way."
"Accountables"	"toString"	"Returns a String description of an Accountable and any nested resources. This is intended for development and debugging."
"Accountables"	"namedAccountable"	"Returns an accountable with the provided description, children and bytes. The resource descriptions are constructed in this format: description [toString()] This is a point-in-time type safe view: consumers will not be able to cast or manipulate the resources in any way, provided that the passed in children Accountables (and all their descendants) were created with one of the namedAccountable functions."
"AfterEffect"	"score"	"Returns the aftereffect score."
"AfterEffect"	"AfterEffect"	"Sole constructor. (For invocation by subclass constructors, typically implicit.)"
"AfterEffect"	"explain"	"Returns an explanation for the score."
"AfterEffect"	"toString"	"Subclasses must override this method to return the code of the after effect formula. Refer to the original paper for the list."
"AfterEffect.NoAfterEffect"	"explain"	"Description copied from class:?AfterEffect Returns an explanation for the score."
"AfterEffect.NoAfterEffect"	"AfterEffect.NoAfterEffect"	"Sole constructor: parameter-free"
"AfterEffect.NoAfterEffect"	"toString"	"Description copied from class:?AfterEffect Subclasses must override this method to return the code of the after effect formula. Refer to the original paper for the list."
"AfterEffect.NoAfterEffect"	"score"	"Description copied from class:?AfterEffect Returns the aftereffect score."
"AfterEffectB"	"toString"	"Description copied from class:?AfterEffect Subclasses must override this method to return the code of the after effect formula. Refer to the original paper for the list."
"AfterEffectB"	"explain"	"Description copied from class:?AfterEffect Returns an explanation for the score."
"AfterEffectB"	"score"	"Description copied from class:?AfterEffect Returns the aftereffect score."
"AfterEffectB"	"AfterEffectB"	"Sole constructor: parameter-free"
"AfterEffectL"	"score"	"Description copied from class:?AfterEffect Returns the aftereffect score."
"AfterEffectL"	"explain"	"Description copied from class:?AfterEffect Returns an explanation for the score."
"AfterEffectL"	"toString"	"Description copied from class:?AfterEffect Subclasses must override this method to return the code of the after effect formula. Refer to the original paper for the list."
"AfterEffectL"	"AfterEffectL"	"Sole constructor: parameter-free"
"Analyzer"	"initReader"	"Override this if you want to add a CharFilter chain. The default implementation returns reader unchanged."
"Analyzer"	"createComponents"	"Creates a new Analyzer.TokenStreamComponents instance for this analyzer."
"Analyzer"	"setVersion"	"Set the version of Lucene this analyzer should mimic the behavior for for analysis."
"Analyzer"	"PER_FIELD_REUSE_STRATEGY"	"A predefined Analyzer.ReuseStrategy that reuses components per-field by maintaining a Map of TokenStreamComponent per field name."
"Analyzer"	"tokenStream"	"Returns a TokenStream suitable for fieldName, tokenizing the contents of text. This method uses createComponents(String) to obtain an instance of Analyzer.TokenStreamComponents. It returns the sink of the components and stores the components internally. Subsequent calls to this method will reuse the previously stored components after resetting them through Analyzer.TokenStreamComponents.setReader(Reader). NOTE: After calling this method, the consumer must follow the workflow described in TokenStream to properly consume its contents. See the Analysis package documentation for some examples demonstrating this."
"Analyzer"	"getPositionIncrementGap"	"Invoked before indexing a IndexableField instance if terms have already been added to that field. This allows custom analyzers to place an automatic position increment gap between IndexbleField instances using the same field name. The default value position increment gap is 0. With a 0 position increment gap and the typical default token position increment of 1, all terms in a field, including across IndexableField instances, are in successive positions, allowing exact PhraseQuery matches, for instance, across IndexableField instance boundaries."
"Analyzer"	"getReuseStrategy"	"Returns the used Analyzer.ReuseStrategy."
"Analyzer"	"getVersion"	"Return the version of Lucene this analyzer will mimic the behavior of for analysis."
"Analyzer"	"getOffsetGap"	"Just like getPositionIncrementGap(java.lang.String), except for Token offsets instead. By default this returns 1. This method is only called if the field produced at least one token for indexing."
"Analyzer"	"close"	"Frees persistent resources used by this Analyzer"
"Analyzer"	"Analyzer"	"Expert: create a new Analyzer with a custom Analyzer.ReuseStrategy. NOTE: if you just want to reuse on a per-field basis, it's easier to use a subclass of AnalyzerWrapper such as PerFieldAnalyerWrapper instead."
"Analyzer"	"GLOBAL_REUSE_STRATEGY"	"A predefined Analyzer.ReuseStrategy that reuses the same components for every field."
"Analyzer.ReuseStrategy"	"setReusableComponents"	"Stores the given TokenStreamComponents as the reusable components for the field with the give name."
"Analyzer.ReuseStrategy"	"Analyzer.ReuseStrategy"	"Sole constructor. (For invocation by subclass constructors, typically implicit.)"
"Analyzer.ReuseStrategy"	"getReusableComponents"	"Gets the reusable TokenStreamComponents for the field with the given name."
"Analyzer.ReuseStrategy"	"getStoredValue"	"Returns the currently stored value."
"Analyzer.ReuseStrategy"	"setStoredValue"	"Sets the stored value."
"Analyzer.TokenStreamComponents"	"sink"	"Sink tokenstream, such as the outer tokenfilter decorating the chain. This can be the source if there are no filters."
"Analyzer.TokenStreamComponents"	"source"	"Original source of the tokens."
"Analyzer.TokenStreamComponents"	"getTokenizer"	"Returns the component's Tokenizer"
"Analyzer.TokenStreamComponents"	"Analyzer.TokenStreamComponents"	"Creates a new Analyzer.TokenStreamComponents instance."
"Analyzer.TokenStreamComponents"	"getTokenStream"	"Returns the sink TokenStream"
"Analyzer.TokenStreamComponents"	"setReader"	"Resets the encapsulated components with the given reader. If the components cannot be reset, an Exception should be thrown."
"AnalyzerWrapper"	"getOffsetGap"	"Description copied from class:?Analyzer Just like Analyzer.getPositionIncrementGap(java.lang.String), except for Token offsets instead. By default this returns 1. This method is only called if the field produced at least one token for indexing."
"AnalyzerWrapper"	"initReader"	"Description copied from class:?Analyzer Override this if you want to add a CharFilter chain. The default implementation returns reader unchanged."
"AnalyzerWrapper"	"wrapComponents"	"Wraps / alters the given TokenStreamComponents, taken from the wrapped Analyzer, to form new components. It is through this method that new TokenFilters can be added by AnalyzerWrappers. By default, the given components are returned."
"AnalyzerWrapper"	"getWrappedAnalyzer"	"Retrieves the wrapped Analyzer appropriate for analyzing the field with the given name"
"AnalyzerWrapper"	"wrapReader"	"Wraps / alters the given Reader. Through this method AnalyzerWrappers can implement initReader(String, Reader). By default, the given reader is returned."
"AnalyzerWrapper"	"createComponents"	"Description copied from class:?Analyzer Creates a new Analyzer.TokenStreamComponents instance for this analyzer."
"AnalyzerWrapper"	"getPositionIncrementGap"	"Description copied from class:?Analyzer Invoked before indexing a IndexableField instance if terms have already been added to that field. This allows custom analyzers to place an automatic position increment gap between IndexbleField instances using the same field name. The default value position increment gap is 0. With a 0 position increment gap and the typical default token position increment of 1, all terms in a field, including across IndexableField instances, are in successive positions, allowing exact PhraseQuery matches, for instance, across IndexableField instance boundaries."
"AnalyzerWrapper"	"AnalyzerWrapper"	"Creates a new AnalyzerWrapper with the given reuse strategy. If you want to wrap a single delegate Analyzer you can probably reuse its strategy when instantiating this subclass: super(delegate.getReuseStrategy());. If you choose different analyzers per field, use Analyzer.PER_FIELD_REUSE_STRATEGY."
"ArrayUtil"	"timSort"	"Sorts the given array in natural order. This method uses the Tim sort algorithm, but falls back to binary sort for small arrays."
"ArrayUtil"	"parseInt"	"Parses the string argument as if it was an int value and returns the result. Throws NumberFormatException if the string does not represent an int quantity. The second argument specifies the radix to use when parsing the value."
"ArrayUtil"	"MAX_ARRAY_LENGTH"	"Maximum length for an array (Integer.MAX_VALUE - RamUsageEstimator.NUM_BYTES_ARRAY_HEADER)."
"ArrayUtil"	"oversize"	"Returns an array size >= minTargetSize, generally over-allocating exponentially to achieve amortized linear-time cost as the array grows. NOTE: this was originally borrowed from Python 2.4.2 listobject.c sources (attribution in LICENSE.txt), but has now been substantially changed based on discussions from java-dev thread with subject ""Dynamic array reallocation algorithms"", started on Jan 12 2010."
"ArrayUtil"	"swap"	"Swap values stored in slots i and j"
"ArrayUtil"	"naturalComparator"	"Get the natural Comparator for the provided object class."
"ArrayUtil"	"introSort"	"Sorts the given array in natural order. This method uses the intro sort algorithm, but falls back to insertion sort for small arrays."
"ArrayUtil"	"hashCode"	"Returns hash of bytes in range start (inclusive) to end (inclusive)"
"ArrayUtil"	"equals"	"See if two array slices are the same."
"AttributeFactory"	"getStaticImplementation"	"Returns an AttributeFactory returning an instance of the given clazz for the attributes it implements. The given clazz must have a public no-arg constructor. For all other attributes it calls the given delegate factory as fallback. This method can be used to prefer a specific AttributeImpl which combines multiple attributes over separate classes. Please save instances created by this method in a static final field, because on each call, this does reflection for creating a MethodHandle."
"AttributeFactory"	"createAttributeInstance"	"Returns an AttributeImpl for the supplied Attribute interface class."
"AttributeFactory"	"DEFAULT_ATTRIBUTE_FACTORY"	"This is the default factory that creates AttributeImpls using the class name of the supplied Attribute interface class by appending Impl to it."
"AttributeFactory.StaticImplementationAttributeFactory"	"AttributeFactory.StaticImplementationAttributeFactory"	"Expert: Creates an AttributeFactory returning clazz as instance for the attributes it implements and for all other attributes calls the given delegate factory."
"AttributeFactory.StaticImplementationAttributeFactory"	"createAttributeInstance"	"Description copied from class:?AttributeFactory Returns an AttributeImpl for the supplied Attribute interface class."
"AttributeFactory.StaticImplementationAttributeFactory"	"createInstance"	"Creates an instance of A."
"AttributeImpl"	"copyTo"	"Copies the values from this Attribute into the passed-in target attribute. The target implementation must support all the Attributes this implementation supports."
"AttributeImpl"	"clear"	"Clears the values in this AttributeImpl and resets it to its default value. If this implementation implements more than one Attribute interface it clears all."
"AttributeImpl"	"clone"	"In most cases the clone is, and should be, deep in order to be able to properly capture the state of all attributes."
"AttributeImpl"	"reflectAsString"	"This method returns the current attribute values as a string in the following format by calling the reflectWith(AttributeReflector) method: iff prependAttClass=true: ""AttributeClass#key=value,AttributeClass#key=value"" iff prependAttClass=false: ""key=value,key=value"""
"AttributeImpl"	"reflectWith"	"This method is for introspection of attributes, it should simply add the key/values this attribute holds to the given AttributeReflector. Implementations look like this (e.g. for a combined attribute implementation): 
   public void reflectWith(AttributeReflector reflector) {
     reflector.reflect(CharTermAttribute.class, ""term"", term());
     reflector.reflect(PositionIncrementAttribute.class, ""positionIncrement"", getPositionIncrement());
   }
 If you implement this method, make sure that for each invocation, the same set of Attribute interfaces and keys are passed to AttributeReflector.reflect(java.lang.Class<? extends org.apache.lucene.util.Attribute>, java.lang.String, java.lang.Object) in the same order, but possibly different values. So don't automatically exclude e.g. null properties! Important for migration to Lucene 6: The default implementation is implemented for backwards compatibility in Lucene 5 and calls AttributeReflector.reflect(java.lang.Class<? extends org.apache.lucene.util.Attribute>, java.lang.String, java.lang.Object) for all non-static fields from the implementing class, using the field name as key and the field value as value. The Attribute class is also determined by reflection. Please note that the default implementation can only handle single-Attribute implementations. Please don't use the default implementation anymore, because it will be made abstract in Lucene 6! See above for implementation example."
"AttributeReflector"	"reflect"	"This method gets called for every property in an AttributeImpl/AttributeSource passing the class name of the Attribute, a key and the actual value. E.g., an invocation of CharTermAttributeImpl.reflectWith(org.apache.lucene.util.AttributeReflector) would call this method once using org.apache.lucene.analysis.tokenattributes.CharTermAttribute.class as attribute class, ""term"" as key and the actual value as a String."
"AttributeSource"	"hasAttributes"	"Returns true, iff this AttributeSource has any attributes"
"AttributeSource"	"restoreState"	"Restores this state by copying the values of all attribute implementations that this state contains into the attributes implementations of the targetStream. The targetStream must contain a corresponding instance for each argument contained in this state (e.g. it is not possible to restore the state of an AttributeSource containing a TermAttribute into a AttributeSource using a Token instance as implementation). Note that this method does not affect attributes of the targetStream that are not contained in this state. In other words, if for example the targetStream contains an OffsetAttribute, but this state doesn't, then the value of the OffsetAttribute remains unchanged. It might be desirable to reset its value to the default, in which case the caller should first call clearAttributes() on the targetStream."
"AttributeSource"	"getAttributeClassesIterator"	"Returns a new iterator that iterates the attribute classes in the same order they were added in."
"AttributeSource"	"getAttributeFactory"	"returns the used AttributeFactory."
"AttributeSource"	"clearAttributes"	"Resets all Attributes in this AttributeSource by calling AttributeImpl.clear() on each Attribute implementation."
"AttributeSource"	"captureState"	"Captures the state of all Attributes. The return value can be passed to restoreState(org.apache.lucene.util.AttributeSource.State) to restore the state of this or another AttributeSource."
"AttributeSource"	"reflectWith"	"This method is for introspection of attributes, it should simply add the key/values this AttributeSource holds to the given AttributeReflector. This method iterates over all Attribute implementations and calls the corresponding AttributeImpl.reflectWith(org.apache.lucene.util.AttributeReflector) method."
"AttributeSource"	"copyTo"	"Copies the contents of this AttributeSource to the given target AttributeSource. The given instance has to provide all Attributes this instance contains. The actual attribute implementations must be identical in both AttributeSource instances; ideally both AttributeSource instances should use the same AttributeFactory. You can use this method as a replacement for restoreState(org.apache.lucene.util.AttributeSource.State), if you use cloneAttributes() instead of captureState()."
"AttributeSource"	"AttributeSource"	"An AttributeSource using the supplied AttributeFactory for creating new Attribute instances."
"AttributeSource"	"reflectAsString"	"This method returns the current attribute values as a string in the following format by calling the reflectWith(AttributeReflector) method: iff prependAttClass=true: ""AttributeClass#key=value,AttributeClass#key=value"" iff prependAttClass=false: ""key=value,key=value"""
"AttributeSource"	"toString"	"Returns a string consisting of the class's simple name, the hex representation of the identity hash code, and the current reflection of all attributes."
"AttributeSource"	"getAttribute"	"Returns the instance of the passed in Attribute contained in this AttributeSource The caller must pass in a Class<? extends Attribute> value."
"AttributeSource"	"cloneAttributes"	"Performs a clone of all AttributeImpl instances returned in a new AttributeSource instance. This method can be used to e.g. create another TokenStream with exactly the same attributes (using AttributeSource(AttributeSource)). You can also use it as a (non-performant) replacement for captureState(), if you need to look into / modify the captured state."
"AttributeSource"	"getAttributeImplsIterator"	"Returns a new iterator that iterates all unique Attribute implementations. This iterator may contain less entries that getAttributeClassesIterator(), if one instance implements more than one Attribute interface."
"AttributeSource"	"addAttributeImpl"	"Expert: Adds a custom AttributeImpl instance with one or more Attribute interfaces. NOTE: It is not guaranteed, that att is added to the AttributeSource, because the provided attributes may already exist. You should always retrieve the wanted attributes using getAttribute(java.lang.Class<T>) after adding with this method and cast to your class. The recommended way to use custom implementations is using an AttributeFactory."
"AttributeSource"	"hasAttribute"	"The caller must pass in a Class<? extends Attribute> value. Returns true, iff this AttributeSource contains the passed-in Attribute."
"AttributeSource"	"addAttribute"	"The caller must pass in a Class<? extends Attribute> value. This method first checks if an instance of that class is already in this AttributeSource and returns it. Otherwise a new instance is created, added to this AttributeSource and returned."
"Automata"	"appendChar"	"Appends the specified character to the specified state, returning a new state."
"Automata"	"makeString"	"Returns a new (deterministic) automaton that accepts the single given string from the specified unicode code points."
"Automata"	"makeStringUnion"	"Returns a new (deterministic and minimal) automaton that accepts the union of the given collection of BytesRefs representing UTF-8 encoded strings."
"Automata"	"makeEmptyString"	"Returns a new (deterministic) automaton that accepts only the empty string."
"Automata"	"appendAnyChar"	"Accept any single character starting from the specified state, returning the new state"
"Automata"	"makeBinaryInterval"	"Creates a new deterministic, minimal automaton accepting all binary terms in the specified interval. Note that unlike makeDecimalInterval(int, int, int), the returned automaton is infinite, because terms behave like floating point numbers leading with a decimal point. However, in the special case where min == max, and both are inclusive, the automata will be finite and accept exactly one term."
"Automata"	"makeAnyString"	"Returns a new (deterministic) automaton that accepts all strings."
"Automata"	"makeBinary"	"Returns a new (deterministic) automaton that accepts the single given binary term."
"Automata"	"makeCharRange"	"Returns a new (deterministic) automaton that accepts a single codepoint whose value is in the given interval (including both end points)."
"Automata"	"makeChar"	"Returns a new (deterministic) automaton that accepts a single codepoint of the given value."
"Automata"	"makeEmpty"	"Returns a new (deterministic) automaton with the empty language."
"Automata"	"makeAnyChar"	"Returns a new (deterministic) automaton that accepts any single codepoint."
"Automata"	"makeAnyBinary"	"Returns a new (deterministic) automaton that accepts all binary terms."
"Automata"	"makeDecimalInterval"	"Returns a new automaton that accepts strings representing decimal (base 10) non-negative integers in the given interval."
"Automaton"	"addTransition"	"Add a new transition with the specified source, dest, min, max."
"Automaton"	"getNextTransition"	"Iterate to the next transition after the provided one"
"Automaton"	"isAccept"	"Returns true if this state is an accept state."
"Automaton"	"initTransition"	"Initialize the provided Transition to iterate through all transitions leaving the specified state. You must call getNextTransition(org.apache.lucene.util.automaton.Transition) to get each transition. Returns the number of transitions leaving this state."
"Automaton"	"getTransition"	"Fill the provided Transition with the index'th transition leaving the specified state."
"Automaton"	"addEpsilon"	"Add a [virtual] epsilon transition between source and dest. Dest state must already have all transitions added because this method simply copies those same transitions over to source."
"Automaton"	"isDeterministic"	"Returns true if this automaton is deterministic (for ever state there is only one transition for each label)."
"Automaton"	"getChildResources"	"Description copied from interface:?Accountable Returns nested resources of this class. The result should be a point-in-time snapshot (to avoid race conditions)."
"Automaton"	"createState"	"Create a new state."
"Automaton"	"getNumTransitions"	"How many transitions this state has."
"Automaton"	"setAccept"	"Set or clear this state as an accept state."
"Automaton"	"copy"	"Copies over all states/transitions from other. The states numbers are sequentially assigned (appended)."
"Automaton"	"getNumStates"	"How many states this automaton has."
"Automaton"	"getSortedTransitions"	"Sugar to get all transitions for all states. This is object-heavy; it's better to iterate state by state instead."
"Automaton"	"toDot"	"Returns the dot (graphviz) representation of this automaton. This is extremely useful for visualizing the automaton."
"Automaton"	"Automaton"	"Constructor which creates an automaton with enough space for the given number of states and transitions."
"Automaton"	"finishState"	"Finishes the current state; call this once you are done adding transitions for a state. This is automatically called if you start adding transitions to a new source state, but for the last state you add you need to this method yourself."
"Automaton"	"step"	"Performs lookup in transitions, assuming determinism."
"Automaton"	"ramBytesUsed"	"Description copied from interface:?Accountable Return the memory usage of this object in bytes. Negative values are illegal."
"Automaton.Builder"	"Automaton.Builder"	"Constructor which creates a builder with enough space for the given number of states and transitions."
"Automaton.Builder"	"isAccept"	"Returns true if this state is an accept state."
"Automaton.Builder"	"copy"	"Copies over all states/transitions from other."
"Automaton.Builder"	"getNumStates"	"How many states this automaton has."
"Automaton.Builder"	"copyStates"	"Copies over all states from other."
"Automaton.Builder"	"finish"	"Compiles all added states and transitions into a new Automaton and returns it."
"Automaton.Builder"	"createState"	"Create a new state."
"Automaton.Builder"	"addTransition"	"Add a new transition with the specified source, dest, min, max."
"Automaton.Builder"	"addEpsilon"	"Add a [virtual] epsilon transition between source and dest. Dest state must already have all transitions added because this method simply copies those same transitions over to source."
"Automaton.Builder"	"setAccept"	"Set or clear this state as an accept state."
"AutomatonProvider"	"getAutomaton"	"Returns automaton of the given name."
"AutomatonQuery"	"toString"	"Description copied from class:?Query Prints a query to a string, with field assumed to be the default field and omitted."
"AutomatonQuery"	"term"	"term containing the field, and possibly some pattern structure"
"AutomatonQuery"	"AutomatonQuery"	"Create a new AutomatonQuery from an Automaton."
"AutomatonQuery"	"automaton"	"the automaton to match index terms against"
"AutomatonQuery"	"getAutomaton"	"Returns the automaton used to create this query"
"AutomatonQuery"	"getTermsEnum"	"Description copied from class:?MultiTermQuery Construct the enumeration to be used, expanding the pattern term. This method should only be called if the field exists (ie, implementations can assume the field does exist). This method should not return null (should instead return TermsEnum.EMPTY if no terms match). The TermsEnum must already be positioned to the first matching term. The given AttributeSource is passed by the MultiTermQuery.RewriteMethod to provide attributes, the rewrite method uses to inform about e.g. maximum competitive boosts. This is currently only used by TopTermsRewrite"
"AutomatonTermsEnum"	"AutomatonTermsEnum"	"Construct an enumerator based upon an automaton, enumerating the specified field, working on a supplied TermsEnum"
"AutomatonTermsEnum"	"nextSeekTerm"	"Description copied from class:?FilteredTermsEnum On the first call to FilteredTermsEnum.next() or if FilteredTermsEnum.accept(org.apache.lucene.util.BytesRef) returns FilteredTermsEnum.AcceptStatus.YES_AND_SEEK or FilteredTermsEnum.AcceptStatus.NO_AND_SEEK, this method will be called to eventually seek the underlying TermsEnum to a new position. On the first call, currentTerm will be null, later calls will provide the term the underlying enum is positioned at. This method returns per default only one time the initial seek term and then null, so no repositioning is ever done. Override this method, if you want a more sophisticated TermsEnum, that repositions the iterator during enumeration. If this method always returns null the enum is empty. Please note: This method should always provide a greater term than the last enumerated term, else the behaviour of this enum violates the contract for TermsEnums."
"AutomatonTermsEnum"	"accept"	"Returns true if the term matches the automaton. Also stashes away the term to assist with smart enumeration."
"BaseCompositeReader"	"readerIndex"	"Helper method for subclasses to get the corresponding reader for a doc ID"
"BaseCompositeReader"	"getSumTotalTermFreq"	"Description copied from class:?IndexReader Returns the sum of TermsEnum.totalTermFreq() for all terms in this field, or -1 if this measure isn't stored by the codec (or if this fields omits term freq and positions). Note that, just like other term measures, this measure does not take deleted documents into account."
"BaseCompositeReader"	"getSumDocFreq"	"Description copied from class:?IndexReader Returns the sum of TermsEnum.docFreq() for all terms in this field, or -1 if this measure isn't stored by the codec. Note that, just like other term measures, this measure does not take deleted documents into account."
"BaseCompositeReader"	"readerBase"	"Helper method for subclasses to get the docBase of the given sub-reader index."
"BaseCompositeReader"	"numDocs"	"Description copied from class:?IndexReader Returns the number of documents in this index."
"BaseCompositeReader"	"BaseCompositeReader"	"Constructs a BaseCompositeReader on the given subReaders."
"BaseCompositeReader"	"getDocCount"	"Description copied from class:?IndexReader Returns the number of documents that have at least one term for this field, or -1 if this measure isn't stored by the codec. Note that, just like other term measures, this measure does not take deleted documents into account."
"BaseCompositeReader"	"totalTermFreq"	"Description copied from class:?IndexReader Returns the total number of occurrences of term across all documents (the sum of the freq() for each doc that has this term). This will be -1 if the codec doesn't support this measure. Note that, like other term measures, this measure does not take deleted documents into account."
"BaseCompositeReader"	"getTermVectors"	"Description copied from class:?IndexReader Retrieve term vectors for this document, or null if term vectors were not indexed. The returned Fields instance acts like a single-document inverted index (the docID will be 0)."
"BaseCompositeReader"	"docFreq"	"Description copied from class:?IndexReader Returns the number of documents containing the term. This method returns 0 if the term or field does not exists. This method does not take into account deleted documents that have not yet been merged away."
"BaseCompositeReader"	"document"	"Description copied from class:?IndexReader Expert: visits the fields of a stored document, for custom processing/loading of each field. If you simply want to load all fields, use IndexReader.document(int). If you want to load a subset, use DocumentStoredFieldVisitor."
"BaseCompositeReader"	"getSequentialSubReaders"	"Description copied from class:?CompositeReader Expert: returns the sequential sub readers that this reader is logically composed of. This method may not return null. NOTE: In contrast to previous Lucene versions this method is no longer public, code that wants to get all LeafReaders this composite is composed of should use IndexReader.leaves()."
"BaseCompositeReader"	"maxDoc"	"Description copied from class:?IndexReader Returns one greater than the largest possible document number. This may be used to, e.g., determine how big to allocate an array which will have an element for every document number in an index."
"BaseDirectory"	"lockFactory"	"Holds the LockFactory instance (implements locking for this Directory instance)."
"BaseDirectory"	"BaseDirectory"	"Sole constructor."
"BaseDirectory"	"obtainLock"	"Description copied from class:?Directory Returns an obtained Lock."
"BasicModel"	"BasicModel"	"Sole constructor. (For invocation by subclass constructors, typically implicit.)"
"BasicModel"	"toString"	"Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicModel"	"score"	"Returns the informative content score."
"BasicModel"	"explain"	"Returns an explanation for the score. Most basic models use the number of documents and the total term frequency to compute Inf1. This method provides a generic explanation for such models. Subclasses that use other statistics must override this method."
"BasicModelBE"	"BasicModelBE"	"Sole constructor: parameter-free"
"BasicModelBE"	"score"	"Description copied from class:?BasicModel Returns the informative content score."
"BasicModelBE"	"toString"	"Description copied from class:?BasicModel Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicModelD"	"toString"	"Description copied from class:?BasicModel Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicModelD"	"BasicModelD"	"Sole constructor: parameter-free"
"BasicModelD"	"score"	"Description copied from class:?BasicModel Returns the informative content score."
"BasicModelG"	"BasicModelG"	"Sole constructor: parameter-free"
"BasicModelG"	"score"	"Description copied from class:?BasicModel Returns the informative content score."
"BasicModelG"	"toString"	"Description copied from class:?BasicModel Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicModelIF"	"BasicModelIF"	"Sole constructor: parameter-free"
"BasicModelIF"	"score"	"Description copied from class:?BasicModel Returns the informative content score."
"BasicModelIF"	"toString"	"Description copied from class:?BasicModel Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicModelIn"	"BasicModelIn"	"Sole constructor: parameter-free"
"BasicModelIn"	"explain"	"Description copied from class:?BasicModel Returns an explanation for the score. Most basic models use the number of documents and the total term frequency to compute Inf1. This method provides a generic explanation for such models. Subclasses that use other statistics must override this method."
"BasicModelIn"	"toString"	"Description copied from class:?BasicModel Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicModelIn"	"score"	"Description copied from class:?BasicModel Returns the informative content score."
"BasicModelIne"	"toString"	"Description copied from class:?BasicModel Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicModelIne"	"score"	"Description copied from class:?BasicModel Returns the informative content score."
"BasicModelIne"	"BasicModelIne"	"Sole constructor: parameter-free"
"BasicModelP"	"LOG2_E"	"log2(Math.E), precomputed."
"BasicModelP"	"BasicModelP"	"Sole constructor: parameter-free"
"BasicModelP"	"score"	"Description copied from class:?BasicModel Returns the informative content score."
"BasicModelP"	"toString"	"Description copied from class:?BasicModel Subclasses must override this method to return the code of the basic model formula. Refer to the original paper for the list."
"BasicStats"	"numberOfDocuments"	"The number of documents."
"BasicStats"	"setNumberOfDocuments"	"Sets the number of documents."
"BasicStats"	"boost"	"For most Similarities, the immediate and the top level query boosts are not handled differently. Hence, this field is just the product of the other two."
"BasicStats"	"numberOfFieldTokens"	"The total number of tokens in the field."
"BasicStats"	"getNumberOfDocuments"	"Returns the number of documents."
"BasicStats"	"setAvgFieldLength"	"Sets the average field length."
"BasicStats"	"getBoost"	"Returns the total boost."
"BasicStats"	"BasicStats"	"Constructor."
"BasicStats"	"setDocFreq"	"Sets the document frequency."
"BasicStats"	"setTotalTermFreq"	"Sets the total number of occurrences of this term across all documents."
"BasicStats"	"getDocFreq"	"Returns the document frequency."
"BasicStats"	"getAvgFieldLength"	"Returns the average field length."
"BasicStats"	"setNumberOfFieldTokens"	"Sets the total number of tokens in the field."
"BasicStats"	"totalTermFreq"	"The total number of occurrences of this term across all documents."
"BasicStats"	"getTotalTermFreq"	"Returns the total number of occurrences of this term across all documents."
"BasicStats"	"normalize"	"No normalization is done. boost is saved in the object, however."
"BasicStats"	"avgFieldLength"	"The average field length."
"BasicStats"	"rawNormalizationValue"	"Computes the raw normalization value. This basic implementation returns the query boost. Subclasses may override this method to include other factors (such as idf), or to save the value for inclusion in normalize(float, float), etc."
"BasicStats"	"getNumberOfFieldTokens"	"Returns the total number of tokens in the field."
"BasicStats"	"getValueForNormalization"	"The square of the raw normalization value."
"BasicStats"	"docFreq"	"The document frequency."
"BinaryDocValues"	"BinaryDocValues"	"Sole constructor. (For invocation by subclass constructors, typically implicit.)"
"BinaryDocValues"	"get"	"Lookup the value for document. The returned BytesRef may be re-used across calls to get(int) so make sure to copy it if you want to keep it around."
"BinaryDocValuesField"	"BinaryDocValuesField"	"Create a new binary DocValues field."
"BinaryDocValuesField"	"TYPE"	"Type for straight bytes DocValues."
"BitDocIdSet"	"bits"	"Description copied from class:?DocIdSet Optionally provides a Bits interface for random access to matching documents."
"BitDocIdSet"	"BitDocIdSet"	"Same as BitDocIdSet(BitSet, long) but uses the set's approximate cardinality as a cost."
"BitDocIdSet"	"ramBytesUsed"	"Description copied from interface:?Accountable Return the memory usage of this object in bytes. Negative values are illegal."
"BitDocIdSet"	"iterator"	"Description copied from class:?DocIdSet Provides a DocIdSetIterator to access the set. This implementation can return null if there are no docs that match."
"BitDocIdSet"	"isCacheable"	"This DocIdSet implementation is cacheable."
"BitDocIdSet.Builder"	"BitDocIdSet.Builder"	"Create a new empty instance."
"BitDocIdSet.Builder"	"build"	"Build a DocIdSet that contains all doc ids that have been added. This method may return null if no documents were addded to this builder. NOTE: this is a destructive operation, the builder should not be used anymore after this method has been called."
"BitDocIdSet.Builder"	"or"	"Add the content of the provided DocIdSetIterator to this builder."
"BitDocIdSet.Builder"	"and"	"Deprecated.? Removes from this builder documents that are not contained in it."
"BitDocIdSet.Builder"	"andNot"	"Deprecated.? Removes from this builder documents that are contained in it."
"BitDocIdSet.Builder"	"isDefinitelyEmpty"	"Is this builder definitely empty? If so, build() will return null. This is usually the same as simply being empty but if this builder was constructed with the full option or if an iterator was passed that iterated over no documents, then we're not sure."
"Bits"	"length"	"Returns the number of bits in this set"
"Bits"	"get"	"Returns the value of the bit with the specified index."
"Bits.MatchAllBits"	"get"	"Description copied from interface:?Bits Returns the value of the bit with the specified index."
"Bits.MatchAllBits"	"length"	"Description copied from interface:?Bits Returns the number of bits in this set"
"Bits.MatchNoBits"	"length"	"Description copied from interface:?Bits Returns the number of bits in this set"
"Bits.MatchNoBits"	"get"	"Description copied from interface:?Bits Returns the value of the bit with the specified index."
"BitSet"	"andNot"	"Deprecated.? this = this AND NOT other. The state of the iterator after this operation terminates is undefined."
"BitSet"	"nextSetBit"	"Returns the index of the first set bit starting at the index specified. DocIdSetIterator.NO_MORE_DOCS is returned if there are no more set bits."
"BitSet"	"assertUnpositioned"	"Assert that the current doc is -1."
"BitSet"	"and"	"Deprecated.? Does in-place AND of the bits provided by the iterator. The state of the iterator after this operation terminates is undefined."
"BitSet"	"clear"	"Clears a range of bits."
"BitSet"	"approximateCardinality"	"Return an approximation of the cardinality of this set. Some implementations may trade accuracy for speed if they have the ability to estimate the cardinality of the set without iterating over all the data. The default implementation returns cardinality()."
"BitSet"	"or"	"Does in-place OR of the bits provided by the iterator. The state of the iterator after this operation terminates is undefined."
"BitSet"	"of"	"Build a BitSet from the content of the provided DocIdSetIterator. NOTE: this will fully consume the DocIdSetIterator."
"BitSet"	"prevSetBit"	"Returns the index of the last set bit before or on the index specified. -1 is returned if there are no more set bits."
"BitSet"	"getChildResources"	"Description copied from interface:?Accountable Returns nested resources of this class. The result should be a point-in-time snapshot (to avoid race conditions)."
"BitSet"	"cardinality"	"Return the number of bits that are set. NOTE: this method is likely to run in linear time"
"BitSet"	"set"	"Set the bit at i."
"BitSetIterator"	"cost"	"Description copied from class:?DocIdSetIterator Returns the estimated cost of this DocIdSetIterator. This is generally an upper bound of the number of documents this iterator might match, but may be a rough heuristic, hardcoded value, or otherwise completely inaccurate."
"BitSetIterator"	"nextDoc"	"Description copied from class:?DocIdSetIterator Advances to the next document in the set and returns the doc it is currently on, or DocIdSetIterator.NO_MORE_DOCS if there are no more docs in the set. NOTE: after the iterator has exhausted you should not call this method, as it may result in unpredicted behavior."
"BitSetIterator"	"advance"	"Description copied from class:?DocIdSetIterator Advances to the first beyond the current whose document number is greater than or equal to target, and returns the document number itself. Exhausts the iterator and returns DocIdSetIterator.NO_MORE_DOCS if target is greater than the highest document number in the set. The behavior of this method is undefined when called with target ¡Ü current, or after the iterator has exhausted. Both cases may result in unpredicted behavior. When target > current it behaves as if written: 
 int advance(int target) {
   int doc;
   while ((doc = nextDoc()) < target) {
   }
   return doc;
 }
 Some implementations are considerably more efficient than that. NOTE: this method may be called with DocIdSetIterator.NO_MORE_DOCS for efficiency by some Scorers. If your implementation cannot efficiently determine that it should exhaust, it is recommended that you check for that value in each call to this method."
"BitSetIterator"	"getFixedBitSetOrNull"	"If the provided iterator wraps a FixedBitSet, returns it, otherwise returns null."
"BitSetIterator"	"docID"	"Description copied from class:?DocIdSetIterator Returns the following: -1 if DocIdSetIterator.nextDoc() or DocIdSetIterator.advance(int) were not called yet. DocIdSetIterator.NO_MORE_DOCS if the iterator has exhausted. Otherwise it should return the doc ID it is currently on."
"BitSetIterator"	"BitSetIterator"	"Sole constructor."
"BitSetIterator"	"getSparseFixedBitSetOrNull"	"If the provided iterator wraps a SparseFixedBitSet, returns it, otherwise returns null."
"BitsFilteredDocIdSet"	"wrap"	"Deprecated.? Convenience wrapper method: If acceptDocs == null it returns the original set without wrapping."
"BitsFilteredDocIdSet"	"BitsFilteredDocIdSet"	"Deprecated.? Constructor."
"BitsFilteredDocIdSet"	"match"	"Deprecated.? Description copied from class:?FilteredDocIdSet Validation method to determine whether a docid should be in the result set."
"BitUtil"	"bitCount"	"Deprecated.?Use Integer.bitCount(int) instead. Return the number of bits sets in b."
"BitUtil"	"nextHighestPowerOfTwo"	"returns the next highest power of two, or the current value if it's already a power of two or zero"
"BitUtil"	"flipFlop"	"flip flops odd with even bits"
"BitUtil"	"pop_andnot"	"Returns the popcount or cardinality of A & ~B. Neither array is modified."
"BitUtil"	"deinterleave"	"Deinterleaves long value back to two concatenated 32bit values"
"BitUtil"	"pop_array"	"Returns the number of set bits in an array of longs."
"BitUtil"	"zigZagDecode"	"Decode a long previously encoded with zigZagEncode(long)."
"BitUtil"	"pop_union"	"Returns the popcount or cardinality of the union of two sets. Neither array is modified."
"BitUtil"	"interleave"	"Interleaves the first 32 bits of each long value Adapted from: http://graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN"
"BitUtil"	"bitList"	"Deprecated.?do not use. Return the list of bits which are set in b encoded as followed: (i >>> (4 * n)) & 0x0F is the offset of the n-th set bit of the given byte plus one, or 0 if there are n or less bits set in the given byte. For example bitList(12) returns 0x43: 0x43 & 0x0F is 3, meaning the the first bit set is at offset 3-1 = 2, (0x43 >>> 4) & 0x0F is 4, meaning there is a second bit set at offset 4-1=3, (0x43 >>> 8) & 0x0F is 0, meaning there is no more bit set in this byte."
"BitUtil"	"pop_intersect"	"Returns the popcount or cardinality of the two sets after an intersection. Neither array is modified."
"BitUtil"	"zigZagEncode"	"Zig-zag encode the provided long. Assuming the input is a signed long whose absolute value can be stored on n bits, the returned value will be an unsigned long that can be stored on n+1 bits."
"BitUtil"	"pop_xor"	"Returns the popcount or cardinality of A ^ B Neither array is modified."
"BlendedTermQuery"	"toString"	"Description copied from class:?Query Prints a query to a string, with field assumed to be the default field and omitted."
"BlendedTermQuery"	"rewrite"	"Description copied from class:?Query Expert: called to re-write queries into primitive queries. For example, a PrefixQuery will be rewritten into a BooleanQuery that consists of TermQuerys."
"BlendedTermQuery"	"DISJUNCTION_MAX_REWRITE"	"BlendedTermQuery.DisjunctionMaxRewrite instance with a tie-breaker of 0.01."
"BlendedTermQuery"	"BOOLEAN_REWRITE"	"A BlendedTermQuery.RewriteMethod that adds all sub queries to a BooleanQuery which has coords disabled. This BlendedTermQuery.RewriteMethod is useful when matching on several fields is considered better than having a good match on a single field."
"BlendedTermQuery.Builder"	"add"	"Expert: Add a Term with the provided boost and context. This method is useful if you already have a TermContext object constructed for the given term."
"BlendedTermQuery.Builder"	"build"	"Build the BlendedTermQuery."
"BlendedTermQuery.Builder"	"BlendedTermQuery.Builder"	"Sole constructor."
"BlendedTermQuery.Builder"	"setRewriteMethod"	"Set the BlendedTermQuery.RewriteMethod. Default is to use BlendedTermQuery.DISJUNCTION_MAX_REWRITE."
"BlendedTermQuery.DisjunctionMaxRewrite"	"BlendedTermQuery.DisjunctionMaxRewrite"	"This BlendedTermQuery.RewriteMethod will create DisjunctionMaxQuery instances that have the provided tie breaker."
"BlendedTermQuery.DisjunctionMaxRewrite"	"rewrite"	"Description copied from class:?BlendedTermQuery.RewriteMethod Merge the provided sub queries into a single Query object."
"BlendedTermQuery.RewriteMethod"	"rewrite"	"Merge the provided sub queries into a single Query object."
"BlendedTermQuery.RewriteMethod"	"BlendedTermQuery.RewriteMethod"	"Sole constructor"
"BlockPackedReader"	"BlockPackedReader"	"Sole constructor."
"BlockPackedReader"	"getChildResources"	"Description copied from interface:?Accountable Returns nested resources of this class. The result should be a point-in-time snapshot (to avoid race conditions)."
"BlockPackedReader"	"ramBytesUsed"	"Description copied from interface:?Accountable Return the memory usage of this object in bytes. Negative values are illegal."
"BlockPackedReader"	"get"	"Description copied from class:?LongValues Get value at index."
"BlockPackedReaderIterator"	"ord"	"Return the offset of the next value to read."
"BlockPackedReaderIterator"	"reset"	"Reset the current reader to wrap a stream of valueCount values contained in in. The block size remains unchanged."
"BlockPackedReaderIterator"	"BlockPackedReaderIterator"	"Sole constructor."
"BlockPackedReaderIterator"	"skip"	"Skip exactly count values."
"BlockPackedReaderIterator"	"next"	"Read between 1 and count values."
"BlockPackedWriter"	"finish"	"Flush all buffered data to disk. This instance is not usable anymore after this method has been called until reset(DataOutput) has been called."
"BlockPackedWriter"	"BlockPackedWriter"	"Sole constructor."
"BlockPackedWriter"	"add"	"Append a new long."
"BlockPackedWriter"	"ord"	"Return the number of values which have been added."
"BlockPackedWriter"	"reset"	"Reset this writer to wrap out. The block size remains unchanged."
"BlockTermState"	"isRealTerm"	"Description copied from class:?TermState Returns true if this term is real (e.g., not an auto-prefix term)."
"BlockTermState"	"termBlockOrd"	"the term's ord in the current block"
"BlockTermState"	"totalTermFreq"	"total number of occurrences of this term"
"BlockTermState"	"docFreq"	"how many docs have this term"
"BlockTermState"	"blockFilePointer"	"fp into the terms dict primary file (_X.tim) that holds this term"
"BlockTermState"	"BlockTermState"	"Sole constructor. (For invocation by subclass constructors, typically implicit.)"
"BlockTermState"	"copyFrom"	"Description copied from class:?TermState Copies the content of the given TermState to this instance"
"BlockTreeTermsReader"	"size"	"Description copied from class:?Fields Returns the number of fields or -1 if the number of distinct field names is unknown. If >= 0, Fields.iterator() will return as many field names."
"BlockTreeTermsReader"	"VERSION_CURRENT"	"Current terms format."
"BlockTreeTermsReader"	"VERSION_START"	"Initial terms format."
"BlockTreeTermsReader"	"VERSION_AUTO_PREFIX_TERMS"	"Auto-prefix terms."
"BlockTreeTermsReader"	"terms"	"Description copied from class:?Fields Get the Terms for this field. This will return null if the field does not exist."
"BlockTreeTermsReader"	"checkIntegrity"	"Description copied from class:?FieldsProducer Checks consistency of this reader. Note that this may be costly in terms of I/O, e.g. may involve computing a checksum value against large data files."
"BlockTreeTermsReader"	"iterator"	"Description copied from class:?Fields Returns an iterator that will step through all fields names. This will not return null."
"BlockTreeTermsReader"	"getChildResources"	"Description copied from interface:?Accountable Returns nested resources of this class. The result should be a point-in-time snapshot (to avoid race conditions)."
"BlockTreeTermsReader"	"VERSION_AUTO_PREFIX_TERMS_COND"	"Conditional auto-prefix terms: we record at write time whether this field did write any auto-prefix terms."
"BlockTreeTermsReader"	"BlockTreeTermsReader"	"Sole constructor."
"BlockTreeTermsReader"	"ramBytesUsed"	"Description copied from interface:?Accountable Return the memory usage of this object in bytes. Negative values are illegal."
"BlockTreeTermsWriter"	"validateAutoPrefixSettings"	"Throws IllegalArgumentException if any of these settings is invalid."
"BlockTreeTermsWriter"	"write"	"Description copied from class:?FieldsConsumer Write all fields, terms and postings. This the ""pull"" API, allowing you to iterate more than once over the postings, somewhat analogous to using a DOM API to traverse an XML tree. Notes: You must compute index statistics, including each Term's docFreq and totalTermFreq, as well as the summary sumTotalTermFreq, sumTotalDocFreq and docCount. You must skip terms that have no docs and fields that have no terms, even though the provided Fields API will expose them; this typically requires lazily writing the field or term until you've actually seen the first term or document. The provided Fields instance is limited: you cannot call any methods that return statistics/counts; you cannot pass a non-null live docs when pulling docs/positions enums."
"BlockTreeTermsWriter"	"DEFAULT_MIN_BLOCK_SIZE"	"Suggested default value for the minItemsInBlock parameter to BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)."
"BlockTreeTermsWriter"	"BlockTreeTermsWriter"	"Create a new writer. The number of items (terms or sub-blocks) per block will aim to be between minItemsPerBlock and maxItemsPerBlock, though in some cases the blocks may be smaller than the min. For DOCS_ONLY fields, this terms dictionary will insert automatically generated prefix terms for common prefixes, as long as each prefix matches at least minItemsInAutoPrefix other terms or prefixes, and at most maxItemsInAutoPrefix other terms or prefixes. Set minItemsInAutoPrefix to 0 to disable auto-prefix terms."
"BlockTreeTermsWriter"	"DEFAULT_MAX_BLOCK_SIZE"	"Suggested default value for the maxItemsInBlock parameter to BlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)."
"BlockTreeTermsWriter"	"validateSettings"	"Throws IllegalArgumentException if any of these settings is invalid."
"BM25Similarity"	"idfExplain"	"Computes a score factor for a phrase. The default implementation sums the idf factor for each term in the phrase."
"BM25Similarity"	"discountOverlaps"	"True if overlap tokens (tokens with a position of increment of zero) are discounted from the document's length."
"BM25Similarity"	"getB"	"Returns the b parameter"
"BM25Similarity"	"computeNorm"	"Description copied from class:?Similarity Computes the normalization value for a field, given the accumulated state of term processing for this field (see FieldInvertState). Matches in longer fields are less precise, so implementations of this method usually set smaller values when state.getLength() is large, and larger values when state.getLength() is small."
"BM25Similarity"	"decodeNormValue"	"The default implementation returns 1 / f2 where f is SmallFloat.byte315ToFloat(byte)."
"BM25Similarity"	"getK1"	"Returns the k1 parameter"
"BM25Similarity"	"encodeNormValue"	"The default implementation encodes boost / sqrt(length) with SmallFloat.floatToByte315(float). This is compatible with Lucene's default implementation. If you change this, then you should change decodeNormValue(byte) to match."
"BM25Similarity"	"BM25Similarity"	"BM25 with these default values: k1 = 1.2, b = 0.75."
"BM25Similarity"	"sloppyFreq"	"Implemented as 1 / (distance + 1)."
"BM25Similarity"	"scorePayload"	"The default implementation returns 1"
"BM25Similarity"	"getDiscountOverlaps"	"Returns true if overlap tokens are discounted from the document's length."
"BM25Similarity"	"computeWeight"	"Description copied from class:?Similarity Compute any collection-level weight (e.g. IDF, average document length, etc) needed for scoring a query."
"BM25Similarity"	"idf"	"Implemented as log(1 + (numDocs - docFreq + 0.5)/(docFreq + 0.5))."
"BM25Similarity"	"setDiscountOverlaps"	"Sets whether overlap tokens (Tokens with 0 position increment) are ignored when computing norm. By default this is true, meaning overlap tokens do not count when computing norms."
"BM25Similarity"	"avgFieldLength"	"The default implementation computes the average as sumTotalTermFreq / maxDoc, or returns 1 if the index does not store sumTotalTermFreq: any field that omits frequency information)."
"BM25Similarity"	"simScorer"	"Description copied from class:?Similarity Creates a new Similarity.SimScorer to score matching documents from a segment of the inverted index."
"BooleanClause"	"setQuery"	"Deprecated.?BooleanClause will be immutable in 6.0. Set the Query."
"BooleanClause"	"hashCode"	"Returns a hash code value for this object."
"BooleanClause"	"BooleanClause"	"Constructs a BooleanClause."
"BooleanClause"	"equals"	"Returns true if o is equal to this."
"BooleanClause"	"setOccur"	"Deprecated.?BooleanClause will be immutable in 6.0. Set the BooleanClause.Occur."
"BooleanClause.Occur"	"FILTER"	"Like MUST except that these clauses do not participate in scoring."
"BooleanClause.Occur"	"values"	"Returns an array containing the constants of this enum type, in the order they are declared. This method may be used to iterate over the constants as follows: 
for (BooleanClause.Occur c : BooleanClause.Occur.values())
?   System.out.println(c);"
"BooleanClause.Occur"	"MUST"	"Use this operator for clauses that must appear in the matching documents."
"BooleanClause.Occur"	"valueOf"	"Returns the enum constant of this type with the specified name. The string must match exactly an identifier used to declare an enum constant in this type. (Extraneous whitespace characters are not permitted.)"
"BooleanClause.Occur"	"SHOULD"	"Use this operator for clauses that should appear in the matching documents. For a BooleanQuery with no MUST clauses one or more SHOULD clauses must match a document for the BooleanQuery to match."
"BooleanClause.Occur"	"MUST_NOT"	"Use this operator for clauses that must not appear in the matching documents. Note that it is not possible to search for queries that only consist of a MUST_NOT clause. These clauses do not contribute to the score of documents."
"BooleanQuery"	"equals"	"Compares the specified object with this boolean query for equality. Returns true if and only if the provided object is also a BooleanQuery, has the same value of isCoordDisabled() has the same value of getMinimumNumberShouldMatch() has the same BooleanClause.Occur.SHOULD clauses, regardless of the order has the same BooleanClause.Occur.MUST clauses, regardless of the order has the same set of BooleanClause.Occur.FILTER clauses, regardless of the order and regardless of duplicates has the same set of BooleanClause.Occur.MUST_NOT clauses, regardless of the order and regardless of duplicates"
"BooleanQuery"	"setMinimumNumberShouldMatch"	"Deprecated.?Boolean queries should be created once with BooleanQuery.Builder and then considered immutable. See BooleanQuery.Builder.setMinimumNumberShouldMatch(int). Set the minimum number of matching SHOULD clauses."
"BooleanQuery"	"createWeight"	"Description copied from class:?Query Expert: Constructs an appropriate Weight implementation for this query. Only implemented by primitive queries, which re-write to themselves."
"BooleanQuery"	"clone"	"Description copied from class:?Query Returns a clone of this query."
"BooleanQuery"	"rewrite"	"Description copied from class:?Query Expert: called to re-write queries into primitive queries. For example, a PrefixQuery will be rewritten into a BooleanQuery that consists of TermQuerys."
"BooleanQuery"	"isCoordDisabled"	"Return whether the coord factor is disabled."
"BooleanQuery"	"setMaxClauseCount"	"Set the maximum number of clauses permitted per BooleanQuery. Default value is 1024."
"BooleanQuery"	"iterator"	"Returns an iterator on the clauses in this query. It implements the Iterable interface to make it possible to do: for (BooleanClause clause : booleanQuery) {}"
"BooleanQuery"	"toString"	"Prints a user-readable version of this query."
"BooleanQuery"	"clauses"	"Return a list of the clauses of this BooleanQuery."
"BooleanQuery"	"BooleanQuery"	"Deprecated.?Use the BooleanQuery.Builder class to build boolean queries. Constructs an empty boolean query. Similarity.coord(int,int) may be disabled in scoring, as appropriate. For example, this score factor does not make sense for most automatically generated queries, like WildcardQuery and FuzzyQuery."
"BooleanQuery"	"add"	"Deprecated.?Boolean queries should be created once with BooleanQuery.Builder and then considered immutable. See BooleanQuery.Builder.add(org.apache.lucene.search.BooleanClause). Adds a clause to a boolean query."
"BooleanQuery"	"getMaxClauseCount"	"Return the maximum number of clauses permitted, 1024 by default. Attempts to add more than the permitted number of clauses cause BooleanQuery.TooManyClauses to be thrown."
"BooleanQuery"	"getMinimumNumberShouldMatch"	"Gets the minimum number of the optional BooleanClauses which must be satisfied."
"BooleanQuery"	"getClauses"	"Deprecated.?Use clauses(). Returns the set of clauses in this query."
"BooleanQuery.Builder"	"BooleanQuery.Builder"	"Sole constructor."
"BooleanQuery.Builder"	"setMinimumNumberShouldMatch"	"Specifies a minimum number of the optional BooleanClauses which must be satisfied. By default no optional clauses are necessary for a match (unless there are no required clauses). If this method is used, then the specified number of clauses is required. Use of this method is totally independent of specifying that any specific clauses are required (or prohibited). This number will only be compared against the number of matching optional clauses."
"BooleanQuery.Builder"	"add"	"Add a new clause to this BooleanQuery.Builder. Note that the order in which clauses are added does not have any impact on matching documents or query performance."
"BooleanQuery.Builder"	"build"	"Create a new BooleanQuery based on the parameters that have been set on this builder."
"BooleanQuery.Builder"	"setDisableCoord"	"Similarity.coord(int,int) may be disabled in scoring, as appropriate. For example, this score factor does not make sense for most automatically generated queries, like WildcardQuery and FuzzyQuery."
"BoostAttribute"	"setBoost"	"Sets the boost in this attribute"
"BoostAttribute"	"getBoost"	"Retrieves the boost, default is 1.0f."
"BoostAttributeImpl"	"getBoost"	"Description copied from interface:?BoostAttribute Retrieves the boost, default is 1.0f."
"BoostAttributeImpl"	"clear"	"Description copied from class:?AttributeImpl Clears the values in this AttributeImpl and resets it to its default value. If this implementation implements more than one Attribute interface it clears all."
"BoostAttributeImpl"	"copyTo"	"Description copied from class:?AttributeImpl Copies the values from this Attribute into the passed-in target attribute. The target implementation must support all the Attributes this implementation supports."
"BoostAttributeImpl"	"reflectWith"	"Description copied from class:?AttributeImpl This method is for introspection of attributes, it should simply add the key/values this attribute holds to the given AttributeReflector. Implementations look like this (e.g. for a combined attribute implementation): 
   public void reflectWith(AttributeReflector reflector) {
     reflector.reflect(CharTermAttribute.class, ""term"", term());
     reflector.reflect(PositionIncrementAttribute.class, ""positionIncrement"", getPositionIncrement());
   }
 If you implement this method, make sure that for each invocation, the same set of Attribute interfaces and keys are passed to AttributeReflector.reflect(java.lang.Class<? extends org.apache.lucene.util.Attribute>, java.lang.String, java.lang.Object) in the same order, but possibly different values. So don't automatically exclude e.g. null properties! Important for migration to Lucene 6: The default implementation is implemented for backwards compatibility in Lucene 5 and calls AttributeReflector.reflect(java.lang.Class<? extends org.apache.lucene.util.Attribute>, java.lang.String, java.lang.Object) for all non-static fields from the implementing class, using the field name as key and the field value as value. The Attribute class is also determined by reflection. Please note that the default implementation can only handle single-Attribute implementations. Please don't use the default implementation anymore, because it will be made abstract in Lucene 6! See above for implementation example."
"BoostAttributeImpl"	"setBoost"	"Description copied from interface:?BoostAttribute Sets the boost in this attribute"
"BoostQuery"	"createWeight"	"Description copied from class:?Query Expert: Constructs an appropriate Weight implementation for this query. Only implemented by primitive queries, which re-write to themselves."
"BoostQuery"	"getQuery"	"Return the wrapped Query."
"BoostQuery"	"rewrite"	"Description copied from class:?Query Expert: called to re-write queries into primitive queries. For example, a PrefixQuery will be rewritten into a BooleanQuery that consists of TermQuerys."
"BoostQuery"	"getBoost"	"Description copied from class:?Query Gets the boost for this clause."
"BoostQuery"	"toString"	"Description copied from class:?Query Prints a query to a string, with field assumed to be the default field and omitted."
"BoostQuery"	"BoostQuery"	"Sole constructor: wrap query in such a way that the produced scores will be boosted by boost."
"BufferedChecksum"	"BufferedChecksum"	"Create a new BufferedChecksum with the specified bufferSize"
"BufferedChecksum"	"DEFAULT_BUFFERSIZE"	"Default buffer size: 256"
"BufferedChecksumIndexInput"	"clone"	"Description copied from class:?IndexInput Returns a clone of this stream. Clones of a stream access the same data, and are positioned at the same point as the stream they were cloned from. Expert: Subclasses must ensure that clones may be positioned at different points in the input from each other and from the stream they were cloned from. Warning: Lucene never closes cloned IndexInputs, it will only call IndexInput.close() on the original object. If you access the cloned IndexInput after closing the original object, any readXXX methods will throw AlreadyClosedException. This method is NOT thread safe, so if the current IndexInput is being used by one thread while clone is called by another, disaster could strike."
"BufferedChecksumIndexInput"	"getFilePointer"	"Description copied from class:?IndexInput Returns the current position in this file, where the next read will occur."
"BufferedChecksumIndexInput"	"getChecksum"	"Description copied from class:?ChecksumIndexInput Returns the current checksum value"
"BufferedChecksumIndexInput"	"close"	"Description copied from class:?IndexInput Closes the stream to further operations."
"BufferedChecksumIndexInput"	"length"	"Description copied from class:?IndexInput The number of bytes in the file."
"BufferedChecksumIndexInput"	"BufferedChecksumIndexInput"	"Creates a new BufferedChecksumIndexInput"
"BufferedChecksumIndexInput"	"readByte"	"Description copied from class:?DataInput Reads and returns a single byte."
"BufferedChecksumIndexInput"	"readBytes"	"Description copied from class:?DataInput Reads a specified number of bytes into an array at the specified offset."
"BufferedChecksumIndexInput"	"slice"	"Description copied from class:?IndexInput Creates a slice of this index input, with the given description, offset, and length. The slice is seeked to the beginning."
"BufferedIndexInput"	"readShort"	"Description copied from interface:?RandomAccessInput Reads a short at the given position in the file"
"BufferedIndexInput"	"readLong"	"Description copied from interface:?RandomAccessInput Reads a long at the given position in the file"
"BufferedIndexInput"	"seek"	"Description copied from class:?IndexInput Sets current position in this file, where the next read will occur."
"BufferedIndexInput"	"flushBuffer"	"Flushes the in-memory buffer to the given output, copying at most numBytes. NOTE: this method does not refill the buffer, however it does advance the buffer position."
"BufferedIndexInput"	"readVLong"	"Description copied from class:?DataInput Reads a long stored in variable-length format. Reads between one and nine bytes. Smaller values take fewer bytes. Negative numbers are not supported. The format is described further in DataOutput.writeVInt(int)."
"BufferedIndexInput"	"clone"	"Description copied from class:?IndexInput Returns a clone of this stream. Clones of a stream access the same data, and are positioned at the same point as the stream they were cloned from. Expert: Subclasses must ensure that clones may be positioned at different points in the input from each other and from the stream they were cloned from. Warning: Lucene never closes cloned IndexInputs, it will only call IndexInput.close() on the original object. If you access the cloned IndexInput after closing the original object, any readXXX methods will throw AlreadyClosedException. This method is NOT thread safe, so if the current IndexInput is being used by one thread while clone is called by another, disaster could strike."
"BufferedIndexInput"	"bufferSize"	"Returns default buffer sizes for the given IOContext"
"BufferedIndexInput"	"MIN_BUFFER_SIZE"	"Minimum buffer size allowed"
"BufferedIndexInput"	"setBufferSize"	"Change the buffer size used by this IndexInput"
"BufferedIndexInput"	"readByte"	"Description copied from interface:?RandomAccessInput Reads a byte at the given position in the file"
"BufferedIndexInput"	"wrap"	"Wraps a portion of another IndexInput with buffering. Please note: This is in most cases ineffective, because it may double buffer!"
"BufferedIndexInput"	"getFilePointer"	"Description copied from class:?IndexInput Returns the current position in this file, where the next read will occur."
"BufferedIndexInput"	"readInt"	"Description copied from interface:?RandomAccessInput Reads an integer at the given position in the file"
"BufferedIndexInput"	"BufferedIndexInput"	"Inits BufferedIndexInput with a specific bufferSize"
"BufferedIndexInput"	"seekInternal"	"Expert: implements seek. Sets current position in this file, where the next readInternal(byte[],int,int) will occur."
"BufferedIndexInput"	"readVInt"	"Description copied from class:?DataInput Reads an int stored in variable-length format. Reads between one and five bytes. Smaller values take fewer bytes. Negative numbers are not supported. The format is described further in DataOutput.writeVInt(int)."
"BufferedIndexInput"	"readBytes"	"Description copied from class:?DataInput Reads a specified number of bytes into an array at the specified offset with control over whether the read should be buffered (callers who have their own buffer should pass in ""false"" for useBuffer). Currently only BufferedIndexInput respects this parameter."
"BufferedIndexInput"	"BUFFER_SIZE"	"Default buffer size set to 1024."
"BufferedIndexInput"	"MERGE_BUFFER_SIZE"	"A buffer size for merges set to 4096."
"BufferedIndexInput"	"readInternal"	"Expert: implements buffer refill. Reads bytes from the current position in the input."
"BufferedIndexInput"	"slice"	"Description copied from class:?IndexInput Creates a slice of this index input, with the given description, offset, and length. The slice is seeked to the beginning."
"BufferedIndexInput"	"getBufferSize"	"Returns buffer size. @see #setBufferSize"
"Builder"	"finish"	"Returns final FST. NOTE: this will return null if nothing is accepted by the FST."
"Builder"	"add"	"Add the next input/output pair. The provided input must be sorted after the previous one according to IntsRef.compareTo(org.apache.lucene.util.IntsRef). It's also OK to add the same input twice in a row with different outputs, as long as Outputs implements the Outputs.merge(T, T) method. Note that input is fully consumed after this method is returned (so caller is free to reuse), but output is not. So if your outputs are changeable (eg ByteSequenceOutputs or IntSequenceOutputs) then you cannot reuse across calls."
"Builder"	"Builder"	"Instantiates an FST/FSA builder with all the possible tuning and construction tweaks. Read parameter documentation carefully."
"Builder.UnCompiledNode"	"depth"	"This node's depth, starting from the automaton root."
"BulkScorer"	"cost"	"Same as DocIdSetIterator.cost() for bulk scorers."
"BulkScorer"	"score"	"Collects matching documents in a range and return an estimation of the next matching document which is on or after max. The return value must be: >= max, DocIdSetIterator.NO_MORE_DOCS if there are no more matches, <= the first matching document that is >= max otherwise. min is the minimum document to be considered for matching. All documents strictly before this value must be ignored. Although max would be a legal return value for this method, higher values might help callers skip more efficiently over non-matching portions of the docID space. For instance, a Scorer-based implementation could look like below: 
 private final Scorer scorer; // set via constructor

 public int score(LeafCollector collector, Bits acceptDocs, int min, int max) throws IOException {
   collector.setScorer(scorer);
   int doc = scorer.docID();
   if (doc < min) {
     doc = scorer.advance(min);
   }
   while (doc < max) {
     if (acceptDocs == null || acceptDocs.get(doc)) {
       collector.collect(doc);
     }
     doc = scorer.nextDoc();
   }
   return doc;
 }"
"ByteArrayDataInput"	"readByte"	"Description copied from class:?DataInput Reads and returns a single byte."
"ByteArrayDataInput"	"readLong"	"Description copied from class:?DataInput Reads eight bytes and returns a long."
"ByteArrayDataInput"	"readVLong"	"Description copied from class:?DataInput Reads a long stored in variable-length format. Reads between one and nine bytes. Smaller values take fewer bytes. Negative numbers are not supported. The format is described further in DataOutput.writeVInt(int)."
"ByteArrayDataInput"	"readInt"	"Description copied from class:?DataInput Reads four bytes and returns an int."
"ByteArrayDataInput"	"readBytes"	"Description copied from class:?DataInput Reads a specified number of bytes into an array at the specified offset."
"ByteArrayDataInput"	"skipBytes"	"Description copied from class:?DataInput Skip over numBytes bytes. The contract on this method is that it should have the same behavior as reading the same number of bytes into a buffer and discarding its content. Negative values of numBytes are not supported."
"ByteArrayDataInput"	"readVInt"	"Description copied from class:?DataInput Reads an int stored in variable-length format. Reads between one and five bytes. Smaller values take fewer bytes. Negative numbers are not supported. The format is described further in DataOutput.writeVInt(int)."
"ByteArrayDataInput"	"readShort"	"Description copied from class:?DataInput Reads two bytes and returns a short."
"ByteArrayDataOutput"	"writeByte"	"Description copied from class:?DataOutput Writes a single byte. The most primitive data type is an eight-bit byte. Files are accessed as sequences of bytes. All other data types are defined as sequences of bytes, so file formats are byte-order independent."
"ByteArrayDataOutput"	"writeBytes"	"Description copied from class:?DataOutput Writes an array of bytes."
"ByteBlockPool"	"append"	"Appends the bytes in the provided BytesRef at the current position."
"ByteBlockPool"	"byteOffset"	"Current head offset"
"ByteBlockPool"	"buffers"	"array of buffers currently used in the pool. Buffers are allocated if needed don't modify this outside of this class."
"ByteBlockPool"	"FIRST_LEVEL_SIZE"	"The first level size for new slices"
"ByteBlockPool"	"nextBuffer"	"Advances the pool to its next buffer. This method should be called once after the constructor to initialize the pool. In contrast to the constructor a reset() call will advance the pool to its first buffer immediately."
"ByteBlockPool"	"byteUpto"	"Where we are in head buffer"
"ByteBlockPool"	"readBytes"	"Reads bytes bytes out of the pool starting at the given offset with the given length into the given byte array at offset off. Note: this method allows to copy across block boundaries."
"ByteBlockPool"	"buffer"	"Current head buffer"
"ByteBlockPool"	"allocSlice"	"Creates a new byte slice with the given starting size and returns the slices offset in the pool."
"ByteBlockPool"	"NEXT_LEVEL_ARRAY"	"An array holding the offset into the LEVEL_SIZE_ARRAY to quickly navigate to the next slice level."
"ByteBlockPool"	"LEVEL_SIZE_ARRAY"	"An array holding the level sizes for byte slices."
"ByteBlockPool"	"reset"	"Expert: Resets the pool to its initial state reusing the first buffer. Calling nextBuffer() is not needed after reset."
"ByteBlockPool"	"newSlice"	"Allocates a new slice with the given size."
"ByteRunAutomaton"	"ByteRunAutomaton"	"expert: if utf8 is true, the input is already byte-based"
"ByteRunAutomaton"	"run"	"Returns true if the given byte array is accepted by this automaton"
"ByteSequenceOutputs"	"write"	"Description copied from class:?Outputs Encode an output value into a DataOutput."
"ByteSequenceOutputs"	"getNoOutput"	"Description copied from class:?Outputs NOTE: this output is compared with == so you must ensure that all methods return the single object if it's really no output"
"ByteSequenceOutputs"	"add"	"Description copied from class:?Outputs Eg add(""foo"", ""bar"") -> ""foobar"""
"ByteSequenceOutputs"	"subtract"	"Description copied from class:?Outputs Eg subtract(""foobar"", ""foo"") -> ""bar"""
"ByteSequenceOutputs"	"common"	"Description copied from class:?Outputs Eg common(""foobar"", ""food"") -> ""foo"""
"ByteSequenceOutputs"	"read"	"Description copied from class:?Outputs Decode an output value previously written with Outputs.write(Object, DataOutput)."
"ByteSequenceOutputs"	"skipOutput"	"Description copied from class:?Outputs Skip the output; defaults to just calling Outputs.read(org.apache.lucene.store.DataInput) and discarding the result."
"ByteSequenceOutputs"	"ramBytesUsed"	"Description copied from class:?Outputs Return memory usage for the provided output."
"BytesRef"	"getUTF8SortedAsUTF16Comparator"	"Deprecated.?This comparator is only a transition mechanism"
"BytesRef"	"hashCode"	"Calculates the hash code as required by TermsHash during indexing. This is currently implemented as MurmurHash3 (32 bit), using the seed from StringHelper.GOOD_FAST_HASH_SEED, but is subject to change from release to release."
"BytesRef"	"offset"	"Offset of first valid byte."
"BytesRef"	"bytesEquals"	"Expert: compares the bytes against another BytesRef, returning true if the bytes are equal."
"BytesRef"	"compareTo"	"Unsigned byte order comparison"
"BytesRef"	"clone"	"Returns a shallow clone of this instance (the underlying bytes are not copied and will be shared by both the returned object and this object."
"BytesRef"	"utf8ToString"	"Interprets stored bytes as UTF8 bytes, returning the resulting string"
"BytesRef"	"deepCopyOf"	"Creates a new BytesRef that points to a copy of the bytes from other The returned BytesRef will have a length of other.length and an offset of zero."
"BytesRef"	"bytes"	"The contents of the BytesRef. Should never be null."
"BytesRef"	"BytesRef"	"Initialize the byte[] from the UTF8 bytes for the provided String."
"BytesRef"	"isValid"	"Performs internal consistency checks. Always returns true (or throws IllegalStateException)"
"BytesRef"	"length"	"Length of used bytes."
"BytesRef"	"toString"	"Returns hex encoded bytes, eg [0x6c 0x75 0x63 0x65 0x6e 0x65]"
"BytesRef"	"EMPTY_BYTES"	"An empty byte array for convenience"
"BytesRefArray"	"get"	"Returns the n'th element of this BytesRefArray"
"BytesRefArray"	"clear"	"Clears this BytesRefArray"
"BytesRefArray"	"iterator"	"Returns a BytesRefIterator with point in time semantics. The iterator provides access to all so far appended BytesRef instances. If a non null Comparator is provided the iterator will iterate the byte values in the order specified by the comparator. Otherwise the order is the same as the values were appended. This is a non-destructive operation."
"BytesRefArray"	"append"	"Appends a copy of the given BytesRef to this BytesRefArray."
"BytesRefArray"	"size"	"Returns the current size of this BytesRefArray"
"BytesRefArray"	"BytesRefArray"	"Creates a new BytesRefArray with a counter to track allocated bytes"
"BytesRefBuilder"	"setLength"	"Set the length."
"BytesRefBuilder"	"copyBytes"	"Replace the content of this builder with the provided bytes. Equivalent to calling clear() and then append(BytesRefBuilder)."
"BytesRefBuilder"	"toBytesRef"	"Build a new BytesRef that has the same content as this buffer."
"BytesRefBuilder"	"bytes"	"Return a reference to the bytes of this builder."
"BytesRefBuilder"	"grow"	"Ensure that this builder can hold at least capacity bytes without resizing."
"BytesRefBuilder"	"setByteAt"	"Set a byte."
"BytesRefBuilder"	"BytesRefBuilder"	"Sole constructor."
"BytesRefBuilder"	"append"	"Append the provided bytes to this builder."
"BytesRefBuilder"	"length"	"Return the number of bytes in this buffer."
"BytesRefBuilder"	"copyChars"	"Replace the content of this buffer with UTF-8 encoded bytes that would represent the provided text."
"BytesRefBuilder"	"get"	"Return a BytesRef that points to the internal content of this builder. Any update to the content of this builder might invalidate the provided ref and vice-versa."
"BytesRefBuilder"	"byteAt"	"Return the byte at the given offset."
"BytesRefBuilder"	"clear"	"Reset this builder to the empty state."
"BytesRefFSTEnum"	"doSeekFloor"	"Seeks to largest term that's <= target."
"BytesRefFSTEnum"	"BytesRefFSTEnum"	"doFloor controls the behavior of advance: if it's true doFloor is true, advance positions to the biggest term before target."
"BytesRefFSTEnum"	"doSeekCeil"	"Seeks to smallest term that's >= target."
"BytesRefFSTEnum"	"seekExact"	"Seeks to exactly this term, returning null if the term doesn't exist. This is faster than using seekFloor(org.apache.lucene.util.BytesRef) or seekCeil(org.apache.lucene.util.BytesRef) because it short-circuits as soon the match is not found."
"BytesRefFSTEnum"	"doSeekExact"	"Seeks to exactly target term."
"BytesRefFSTEnum"	"seekFloor"	"Seeks to biggest term that's <= target."
"BytesRefFSTEnum"	"seekCeil"	"Seeks to smallest term that's >= target."
"BytesRefFSTEnum"	"rewindPrefix"	"Rewinds enum state to match the shared prefix between current term and target term"
"BytesRefHash"	"clear"	"Clears the BytesRef which maps to the given BytesRef"
"BytesRefHash"	"add"	"Adds a new BytesRef"
"BytesRefHash"	"byteStart"	"Returns the bytesStart offset into the internally used ByteBlockPool for the given bytesID"
"BytesRefHash"	"addByPoolOffset"	"Adds a ""arbitrary"" int offset instead of a BytesRef term. This is used in the indexer to hold the hash for term vectors, because they do not redundantly store the byte[] term directly and instead reference the byte[] term already stored by the postings BytesRefHash. See add(int textStart) in TermsHashPerField."
"BytesRefHash"	"sort"	"Returns the values array sorted by the referenced byte values. Note: This is a destructive operation. clear() must be called in order to reuse this BytesRefHash instance."
"BytesRefHash"	"close"	"Closes the BytesRefHash and releases all internally used memory"
"BytesRefHash"	"find"	"Returns the id of the given BytesRef."
"BytesRefHash"	"reinit"	"reinitializes the BytesRefHash after a previous clear() call. If clear() has not been called previously this method has no effect."
"BytesRefHash"	"size"	"Returns the number of BytesRef values in this BytesRefHash."
"BytesRefHash"	"BytesRefHash"	"Creates a new BytesRefHash"
"BytesRefHash"	"get"	"Populates and returns a BytesRef with the bytes for the given bytesID. Note: the given bytesID must be a positive integer less than the current size (size())"
"BytesRefHash.BytesStartArray"	"grow"	"Grows the BytesRefHash.BytesStartArray"
"BytesRefHash.BytesStartArray"	"init"	"Initializes the BytesStartArray. This call will allocate memory"
"BytesRefHash.BytesStartArray"	"clear"	"clears the BytesRefHash.BytesStartArray and returns the cleared instance."
"BytesRefHash.BytesStartArray"	"bytesUsed"	"A Counter reference holding the number of bytes used by this BytesRefHash.BytesStartArray. The BytesRefHash uses this reference to track it memory usage"
"BytesRefHash.DirectBytesStartArray"	"clear"	"Description copied from class:?BytesRefHash.BytesStartArray clears the BytesRefHash.BytesStartArray and returns the cleared instance."
"BytesRefHash.DirectBytesStartArray"	"bytesUsed"	"Description copied from class:?BytesRefHash.BytesStartArray A Counter reference holding the number of bytes used by this BytesRefHash.BytesStartArray. The BytesRefHash uses this reference to track it memory usage"
"BytesRefHash.DirectBytesStartArray"	"init"	"Description copied from class:?BytesRefHash.BytesStartArray Initializes the BytesStartArray. This call will allocate memory"
"BytesRefHash.DirectBytesStartArray"	"grow"	"Description copied from class:?BytesRefHash.BytesStartArray Grows the BytesRefHash.BytesStartArray"
"BytesRefIterator"	"EMPTY"	"Singleton BytesRefIterator that iterates over 0 BytesRefs."
"BytesRefIterator"	"next"	"Increments the iteration to the next BytesRef in the iterator. Returns the resulting BytesRef or null if the end of the iterator is reached. The returned BytesRef may be re-used across calls to next. After this method returns null, do not call it again: the results are undefined."
"BytesTermAttribute"	"setBytesRef"	"Sets the BytesRef of the term"
"BytesTermAttributeImpl"	"copyTo"	"Description copied from class:?AttributeImpl Copies the values from this Attribute into the passed-in target attribute. The target implementation must support all the Attributes this implementation supports."
"BytesTermAttributeImpl"	"reflectWith"	"Description copied from class:?AttributeImpl This method is for introspection of attributes, it should simply add the key/values this attribute holds to the given AttributeReflector. Implementations look like this (e.g. for a combined attribute implementation): 
   public void reflectWith(AttributeReflector reflector) {
     reflector.reflect(CharTermAttribute.class, ""term"", term());
     reflector.reflect(PositionIncrementAttribute.class, ""positionIncrement"", getPositionIncrement());
   }
 If you implement this method, make sure that for each invocation, the same set of Attribute interfaces and keys are passed to AttributeReflector.reflect(java.lang.Class<? extends org.apache.lucene.util.Attribute>, java.lang.String, java.lang.Object) in the same order, but possibly different values. So don't automatically exclude e.g. null properties! Important for migration to Lucene 6: The default implementation is implemented for backwards compatibility in Lucene 5 and calls AttributeReflector.reflect(java.lang.Class<? extends org.apache.lucene.util.Attribute>, java.lang.String, java.lang.Object) for all non-static fields from the implementing class, using the field name as key and the field value as value. The Attribute class is also determined by reflection. Please note that the default implementation can only handle single-Attribute implementations. Please don't use the default implementation anymore, because it will be made abstract in Lucene 6! See above for implementation example."
"BytesTermAttributeImpl"	"setBytesRef"	"Description copied from interface:?BytesTermAttribute Sets the BytesRef of the term"
"BytesTermAttributeImpl"	"BytesTermAttributeImpl"	"Initialize this attribute with no bytes."
"BytesTermAttributeImpl"	"clone"	"Description copied from class:?AttributeImpl In most cases the clone is, and should be, deep in order to be able to properly capture the state of all attributes."
"BytesTermAttributeImpl"	"clear"	"Description copied from class:?AttributeImpl Clears the values in this AttributeImpl and resets it to its default value. If this implementation implements more than one Attribute interface it clears all."
"BytesTermAttributeImpl"	"getBytesRef"	"Description copied from interface:?TermToBytesRefAttribute Retrieve this attribute's BytesRef. The bytes are updated from the current term. The implementation may return a new instance or keep the previous one."
"CachingCollector"	"isCached"	"Return true is this collector is able to replay collection."
"CachingCollector"	"replay"	"Replays the cached doc IDs (and scores) to the given Collector. If this instance does not cache scores, then Scorer is not set on other.setScorer as well as scores are not replayed."
"CachingCollector"	"create"	"Create a new CachingCollector that wraps the given collector and caches documents and scores up to the specified max docs threshold."
"CachingTokenFilter"	"incrementToken"	"The first time called, it'll read and cache all tokens from the input."
"CachingTokenFilter"	"CachingTokenFilter"	"Create a new CachingTokenFilter around input. As with any normal TokenFilter, do not call reset on the input; this filter will do it normally."
"CachingTokenFilter"	"isCached"	"If the underlying token stream was consumed and cached."
"CachingTokenFilter"	"end"	"Description copied from class:?TokenFilter This method is called by the consumer after the last token has been consumed, after TokenStream.incrementToken() returned false (using the new TokenStream API). Streams implementing the old API should upgrade to use this feature. This method can be used to perform any end-of-stream operations, such as setting the final offset of a stream. The final offset of a stream might differ from the offset of the last token eg in case one or more whitespaces followed after the last token, but a WhitespaceTokenizer was used. Additionally any skipped positions (such as those removed by a stopfilter) can be applied to the position increment, or any adjustment of other attributes where the end-of-stream value may be important. If you override this method, always call super.end(). NOTE: The default implementation chains the call to the input TokenStream, so be sure to call super.end() first when overriding this method."
"CachingTokenFilter"	"reset"	"Propagates reset if incrementToken has not yet been called. Otherwise it rewinds the iterator to the beginning of the cached list."
"CachingWrapperFilter"	"docIdSetToCache"	"Deprecated.? Provide the DocIdSet to be cached, using the DocIdSet provided by the wrapped Filter. This implementation returns the given DocIdSet, if DocIdSet.isCacheable() returns true, else it calls cacheImpl(DocIdSetIterator, org.apache.lucene.index.LeafReader) Note: This method returns DocIdSet.EMPTY if the given docIdSet is null or if DocIdSet.iterator() return null. The empty instance is use as a placeholder in the cache instead of the null value."
"CachingWrapperFilter"	"getDocIdSet"	"Deprecated.? Description copied from class:?Filter Creates a DocIdSet enumerating the documents that should be permitted in search results. NOTE: null can be returned if no documents are accepted by this Filter. Note: This method will be called once per segment in the index during searching. The returned DocIdSet must refer to document IDs for that segment, not for the top-level reader."
"CachingWrapperFilter"	"ramBytesUsed"	"Deprecated.? Description copied from interface:?Accountable Return the memory usage of this object in bytes. Negative values are illegal."
"CachingWrapperFilter"	"getFilter"	"Deprecated.? Gets the contained filter."
"CachingWrapperFilter"	"cacheImpl"	"Deprecated.? Default cache implementation: uses RoaringDocIdSet."
"CachingWrapperFilter"	"toString"	"Deprecated.? Description copied from class:?Query Prints a query to a st